{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_helper import timeserie2image, read_files\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataset import UCIHARDataset\n",
    "from transform import Transform\n",
    "import torch\n",
    "from barlowtwins import BarlowTwins\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.trainer import Trainer\n",
    "import datetime\n",
    "\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_y, validation_data, validation_y, test_data, test_y = read_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6238</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6239</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6240</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6241</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6242</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2442 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "0     5\n",
       "1     5\n",
       "2     5\n",
       "3     5\n",
       "4     5\n",
       "...  ..\n",
       "6238  2\n",
       "6239  2\n",
       "6240  2\n",
       "6241  2\n",
       "6242  2\n",
       "\n",
       "[2442 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7347</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7348</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7349</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7350</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7351</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4910 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "688   5\n",
       "689   5\n",
       "690   5\n",
       "691   5\n",
       "692   5\n",
       "...  ..\n",
       "7347  2\n",
       "7348  2\n",
       "7349  2\n",
       "7350  2\n",
       "7351  2\n",
       "\n",
       "[4910 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "for i in range(train_data.shape[0]):\n",
    "    signal = train_data.iloc[i,:].values.reshape(9, -1)\n",
    "    image = timeserie2image(signal)\n",
    "    image = np.array([image, image, image])\n",
    "    train_x.append(image)\n",
    "train_x = torch.tensor(np.array(train_x))\n",
    "\n",
    "validation_x = []\n",
    "for i in range(validation_data.shape[0]):\n",
    "    signal = validation_data.iloc[i,:].values.reshape(9, -1)\n",
    "    image = timeserie2image(signal)\n",
    "    image = np.array([image, image, image])\n",
    "    validation_x.append(image)\n",
    "validation_x = torch.tensor(np.array(validation_x))\n",
    "\n",
    "test_x = []\n",
    "for i in range(test_data.shape[0]):\n",
    "    signal = test_data.iloc[i,:].values.reshape(9, -1)\n",
    "    image = timeserie2image(signal)\n",
    "    image = np.array([image, image, image])\n",
    "    test_x.append(image)\n",
    "test_x = torch.tensor(np.array(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = UCIHARDataset(train_x, train_y, transform=Transform())\n",
    "val_dataset = UCIHARDataset(validation_x, validation_y, transform=Transform())\n",
    "test_dataset = UCIHARDataset(test_x, test_y, transform=Transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model = BarlowTwins()\n",
    "current_date = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\dpalo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                      | Params\n",
      "--------------------------------------------------------------\n",
      "0 | backbone        | Sequential                | 11.2 M\n",
      "1 | projection_head | BarlowTwinsProjectionHead | 9.4 M \n",
      "2 | criterion       | BarlowTwinsLoss           | 0     \n",
      "--------------------------------------------------------------\n",
      "20.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "20.6 M    Total params\n",
      "82.496    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c143b734d034c9a8e7defaec261574a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dpalo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\dpalo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\dpalo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6bfff2a2a64a92bfbe5b7c31ec47fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dpalo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\lightning\\pytorch\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping('val_loss', patience=100, verbose=True, mode='min')\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',  # Monitor validation loss\n",
    "    mode='min',          # 'min' mode means the checkpoint will be saved when the monitored quantity decreases\n",
    "    save_top_k=1,        # Save the best model\n",
    "    dirpath=current_date,  # Directory to save the checkpoints\n",
    "    filename='model',  # Filename format\n",
    ")\n",
    "trainer = Trainer(limit_train_batches=1.0, max_epochs=100000, callbacks=[early_stopping, checkpoint_callback], accelerator=\"gpu\", devices=\"auto\")\n",
    "trainer.fit(model=bt_model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_dir = 'data/UCIHAR/dataset/UCI HAR Dataset'\n",
    "# Training data\n",
    "# Train users\n",
    "train_users = pd.read_csv(f'{base_dir}/train/subject_train.txt', header=None)\n",
    "# Train accelerometer data\n",
    "train_acc_x = pd.read_csv(f'{base_dir}/train/Inertial Signals/body_acc_x_train.txt', delim_whitespace=True, header=None)\n",
    "train_acc_y = pd.read_csv(f'{base_dir}/train/Inertial Signals/body_acc_y_train.txt', delim_whitespace=True, header=None)\n",
    "train_acc_z = pd.read_csv(f'{base_dir}/train/Inertial Signals/body_acc_z_train.txt', delim_whitespace=True, header=None)\n",
    "# Train gyroscope data\n",
    "train_gyro_x = pd.read_csv(f'{base_dir}/train/Inertial Signals/body_gyro_x_train.txt', delim_whitespace=True, header=None)\n",
    "train_gyro_y = pd.read_csv(f'{base_dir}/train/Inertial Signals/body_gyro_y_train.txt', delim_whitespace=True, header=None)\n",
    "train_gyro_z = pd.read_csv(f'{base_dir}/train/Inertial Signals/body_gyro_z_train.txt', delim_whitespace=True, header=None)\n",
    "# Train total acc data\n",
    "train_total_acc_x = pd.read_csv(f'{base_dir}/train/Inertial Signals/total_acc_x_train.txt', delim_whitespace=True, header=None)\n",
    "train_total_acc_y = pd.read_csv(f'{base_dir}/train/Inertial Signals/total_acc_y_train.txt', delim_whitespace=True, header=None)\n",
    "train_total_acc_z = pd.read_csv(f'{base_dir}/train/Inertial Signals/total_acc_z_train.txt', delim_whitespace=True, header=None)\n",
    "\n",
    "# Train labels\n",
    "train_y = pd.read_csv(f'{base_dir}/train/y_train.txt', header=None)\n",
    "\n",
    "# Test data\n",
    "# Test users\n",
    "test_users = pd.read_csv(f'{base_dir}/test/subject_test.txt', header=None)\n",
    "# Test accelerometer data\n",
    "test_acc_x = pd.read_csv(f'{base_dir}/test/Inertial Signals/body_acc_x_test.txt', delim_whitespace=True, header=None)\n",
    "test_acc_y = pd.read_csv(f'{base_dir}/test/Inertial Signals/body_acc_y_test.txt', delim_whitespace=True, header=None)\n",
    "test_acc_z = pd.read_csv(f'{base_dir}/test/Inertial Signals/body_acc_z_test.txt', delim_whitespace=True, header=None)\n",
    "# Test gyroscope data\n",
    "test_gyro_x = pd.read_csv(f'{base_dir}/test/Inertial Signals/body_gyro_x_test.txt', delim_whitespace=True, header=None)\n",
    "test_gyro_y = pd.read_csv(f'{base_dir}/test/Inertial Signals/body_gyro_y_test.txt', delim_whitespace=True, header=None)\n",
    "test_gyro_z = pd.read_csv(f'{base_dir}/test/Inertial Signals/body_gyro_z_test.txt', delim_whitespace=True, header=None)\n",
    "# Test total acc data\n",
    "test_total_acc_x = pd.read_csv(f'{base_dir}/test/Inertial Signals/total_acc_x_test.txt', delim_whitespace=True, header=None)\n",
    "test_total_acc_y = pd.read_csv(f'{base_dir}/test/Inertial Signals/total_acc_y_test.txt', delim_whitespace=True, header=None)\n",
    "test_total_acc_z = pd.read_csv(f'{base_dir}/test/Inertial Signals/total_acc_z_test.txt', delim_whitespace=True, header=None)\n",
    "# Test labels\n",
    "test_y = pd.read_csv(f'{base_dir}/test/y_test.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([train_users, train_gyro_x, train_gyro_y, train_gyro_z, train_total_acc_x, train_total_acc_y, train_total_acc_z, train_acc_x, train_acc_y, train_acc_z], axis=1)\n",
    "test_data = pd.concat([test_users, test_gyro_x, test_gyro_y, test_gyro_z, test_total_acc_x, test_total_acc_y, test_total_acc_z, test_acc_x, test_acc_y, test_acc_z], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7352, 1), (2947, 1), (7352, 1153), (2947, 1153))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_users.shape, test_users.shape, train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 68)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_elem = train_data.iloc[0,1:]\n",
    "first_elem = first_elem.values.reshape(9, -1)\n",
    "first_elem[:,:68].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 27 25  3 15  8 19] [ 5  6  7 11 14 16 17 21 22 23 26 28 29 30]\n"
     ]
    }
   ],
   "source": [
    "users_for_train = np.random.choice(train_users.iloc[:,0].unique(), 7, replace=False)\n",
    "users_for_validation = np.setdiff1d(train_users.iloc[:,0].unique(), users_for_train)\n",
    "print(users_for_train, users_for_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dpalo\\AppData\\Local\\Temp\\ipykernel_26204\\3781707170.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  validation_y = train_y[train_data.iloc[:,0].isin(users_for_validation)]\n"
     ]
    }
   ],
   "source": [
    "train_y = train_y[train_data.iloc[:,0].isin(users_for_train)]\n",
    "validation_y = train_y[train_data.iloc[:,0].isin(users_for_validation)]\n",
    "\n",
    "validation_data = train_data[train_data.iloc[:,0].isin(users_for_validation)].iloc[:,1:]\n",
    "train_data = train_data[train_data.iloc[:,0].isin(users_for_train)].iloc[:,1:]\n",
    "test_data = test_data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = []\n",
    "for i in range(train_data.shape[0]):\n",
    "    signal = train_data.iloc[i,:].values.reshape(9, -1)\n",
    "    image = timeserie2image(signal)\n",
    "    image = np.array([image, image, image])\n",
    "    train_x.append(image)\n",
    "train_x = torch.tensor(np.array(train_x))\n",
    "\n",
    "validation_x = []\n",
    "for i in range(validation_data.shape[0]):\n",
    "    signal = validation_data.iloc[i,:].values.reshape(9, -1)\n",
    "    image = timeserie2image(signal)\n",
    "    image = np.array([image, image, image])\n",
    "    validation_x.append(image)\n",
    "validation_x = torch.tensor(np.array(validation_x))\n",
    "\n",
    "test_x = []\n",
    "for i in range(test_data.shape[0]):\n",
    "    signal = test_data.iloc[i,:].values.reshape(9, -1)\n",
    "    image = timeserie2image(signal)\n",
    "    image = np.array([image, image, image])\n",
    "    test_x.append(image)\n",
    "test_x = torch.tensor(np.array(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2442, 3, 36, 128]),\n",
       " torch.Size([4910, 3, 36, 128]),\n",
       " torch.Size([2947, 3, 36, 128]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, validation_x.shape, test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = UCIHARDataset(train_x, train_y, transform=Transform())\n",
    "val_dataset = UCIHARDataset(validation_x, validation_y, transform=Transform())\n",
    "test_dataset = UCIHARDataset(test_x, test_y, transform=Transform())\n",
    "nextval = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 224, 224]), torch.Size([3, 224, 224]), 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nextval[0].shape, nextval[1].shape, nextval[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 5\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 5\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 5\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 5\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 5\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 5\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 5\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 5\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 5\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 5\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 5\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 5\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 5\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 5\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 5\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 5\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 5\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 5\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 5\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 5\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 5\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 5\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 5\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 5\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 5\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 5\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 5\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 4\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 4\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 4\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 4\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 4\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 4\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 4\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 4\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 4\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 4\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 4\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 4\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 4\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 4\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 4\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 4\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 4\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 4\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 4\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 4\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 4\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 4\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 4\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 4\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 6\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 6\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 6\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 6\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 6\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 6\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 6\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 6\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 6\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 6\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 6\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 6\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 6\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 6\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 6\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 6\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 6\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 6\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 6\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 6\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 6\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 6\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 6\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 6\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 6\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 6\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 6\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 1\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 1\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 1\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 1\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 1\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 1\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 1\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 1\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 1\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 1\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 1\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 1\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 1\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 1\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 1\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 1\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 1\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 1\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 1\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 1\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 1\n",
      "SAMPLE SHAPE torch.Size([3, 36, 128])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224]) 1\n"
     ]
    }
   ],
   "source": [
    "iterd = iter(train_dataset)\n",
    "for i in range(100):\n",
    "    nextval = next(iterd)\n",
    "    print(nextval[0].shape, nextval[1].shape, nextval[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, ToPILImage, ToTensor, Normalize\n",
    "# Transform tensor to 224x224\n",
    "transform = Compose([\n",
    "    ToPILImage(),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Resize to 224x224\n",
    "train_dataset.transform = transform\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2433\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1151 into shape (9,newaxis)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(elem)\n\u001b[0;32m     17\u001b[0m data \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39miloc[elem,\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m---> 18\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_val[train_y\u001b[38;5;241m.\u001b[39miloc[elem,\u001b[38;5;241m0\u001b[39m]])\n\u001b[0;32m     20\u001b[0m timeserie2image(data[:,:timestamps], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_val[train_y\u001b[38;5;241m.\u001b[39miloc[elem,\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00melem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 1151 into shape (9,newaxis)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "y_val = {\n",
    "1: 'WALKING',\n",
    "2: 'WALKING_UPSTAIRS',\n",
    "3: 'WALKING_DOWNSTAIRS',\n",
    "4: 'SITTING',\n",
    "5: 'STANDING',\n",
    "6: 'LAYING'\n",
    "}\n",
    "timestamps = 128\n",
    "os.makedirs('images', exist_ok=True)\n",
    "random_elements = np.random.randint(0, train_data.shape[0], 10)\n",
    "for elem in random_elements:\n",
    "    print(elem)\n",
    "    data = train_data.iloc[elem,1:]\n",
    "    data = data.values.reshape(9, -1)\n",
    "    print(y_val[train_y.iloc[elem,0]])\n",
    "    timeserie2image(data[:,:timestamps], f'images/{timestamps}-{y_val[train_y.iloc[elem,0]]}-{elem}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 68)\n"
     ]
    }
   ],
   "source": [
    "timeserie2image(first_elem[:,:68], 'test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 27 25  3 15  8 19] [ 5  6  7 11 14 16 17 21 22 23 26 28 29 30]\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['1',   0,   1,   2,   3,   4,   5,   6,   7,   8,\n",
       "       ...\n",
       "       118, 119, 120, 121, 122, 123, 124, 125, 126, 127],\n",
       "      dtype='object', length=769)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = train_data[train_data.iloc[:,0].isin(users_for_validation)]\n",
    "train_data = train_data[train_data.iloc[:,0].isin(users_for_train)]\n",
    "# Remove users from the data\n",
    "train_data = train_data.iloc[:,1:]\n",
    "validation_data = validation_data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the labels\n",
    "train_y = train_y[train_y.iloc[:,0].isin(users_for_train)]\n",
    "validation_y = train_y[train_y.iloc[:,0].isin(users_for_validation)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [5]\n",
       "Index: []"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7351, 1), (7351, 128), (7351, 128), (7351, 1), (7351, 128))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.shape, train_acc_x.shape, train_acc_y.shape, y_train.shape, train_total.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
